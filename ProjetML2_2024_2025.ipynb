{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoltanReza/classification/blob/dev/ProjetML2_2024_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccUTUEh5Xese"
      },
      "source": [
        "# Prérequis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEVOHu-UcrOU"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1zULJ9daqP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5c072d-aff8-4675-a2b0-ec1424c52a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/ML_FDS\n",
            "Requirement already satisfied: umap-learn[plot] in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.61.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (3.10.0)\n",
            "Requirement already satisfied: datashader in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.17.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (3.6.3)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.20.1)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.13.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.25.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn[plot]) (0.44.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn[plot]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn[plot]) (3.5.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (1.3.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (24.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (2025.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->umap-learn[plot]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->umap-learn[plot]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->umap-learn[plot]) (2025.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (2.2.0)\n",
            "Requirement already satisfied: pyct in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (0.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (2.32.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (2025.1.2)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.11/dist-packages (from holoviews->umap-learn[plot]) (1.6.1)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.11/dist-packages (from holoviews->umap-learn[plot]) (3.0.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (3.2.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (3.0.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->umap-learn[plot]) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (2025.1.31)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.0->holoviews->umap-learn[plot]) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.0->holoviews->umap-learn[plot]) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.0->holoviews->umap-learn[plot]) (0.1.2)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.11/dist-packages (from holoviews) (3.6.3)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from holoviews) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from holoviews) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from holoviews) (24.2)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.11/dist-packages (from holoviews) (2.2.2)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.11/dist-packages (from holoviews) (1.6.1)\n",
            "Requirement already satisfied: param<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from holoviews) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.11/dist-packages (from holoviews) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (1.3.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (2025.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->holoviews) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->holoviews) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->holoviews) (2025.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (0.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh>=3.1->holoviews) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3->holoviews) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.0->holoviews) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.0->holoviews) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.0->holoviews) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (2025.1.31)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (6.17.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd\n",
        "\n",
        "\n",
        "!pip install umap-learn[plot]\n",
        "!pip install holoviews\n",
        "!pip install  ipykernel\n",
        "!pip install joblib\n",
        "# eventuellement ne pas oublier de relancer le kernel du notebook\n",
        "# !pip install tensorflow==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "outputs": [],
      "source": [
        "# Importation des différentes librairies utiles pour le notebook\n",
        "\n",
        "#Sickit learn met régulièrement à jour des versions et\n",
        "#indique des futurs warnings.\n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# librairies générales\n",
        "import joblib\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import auth\n",
        "import google.auth\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "# TensorFlow et keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import glob\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
        "import io\n",
        "import tempfile\n",
        "from skimage.feature import hog\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWgks_L2XyT4"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n70qVaRWwII"
      },
      "outputs": [],
      "source": [
        "# afficher les res de train avec des plots ( version améliorée)\n",
        "def plot_curves_confusion (history,confusion_matrix,class_names):\n",
        "  plt.figure(1,figsize=(16,6))\n",
        "  plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "\n",
        "  # division de la fenêtre graphique en 1 ligne, 3 colonnes,\n",
        "  # graphique en position 1 - loss fonction\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "  # graphique en position 2 - accuracy\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "\n",
        "  # matrice de correlation\n",
        "  plt.subplot(1,3,3)\n",
        "  sns.heatmap(confusion_matrix,annot=True,fmt=\"d\",cmap='Blues',xticklabels=class_names, yticklabels=class_names)# label=class_names)\n",
        "  # labels, title and ticks\n",
        "  plt.xlabel('Predicted', fontsize=12)\n",
        "  #plt.set_label_position('top')\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  #plt.tick_top()\n",
        "  plt.title(\"Correlation matrix\")\n",
        "  plt.ylabel('True', fontsize=12)\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_curves(histories):\n",
        "    plt.figure(1,figsize=(16,6))\n",
        "    plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "    for i in range(len(histories)):\n",
        "    \t# plot loss\n",
        "    \tplt.subplot(121)\n",
        "    \tplt.title('Cross Entropy Loss')\n",
        "    \tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_loss'], color='red', label='test')\n",
        "    \tplt.ylabel('loss')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "    \t# plot accuracy\n",
        "    \tplt.subplot(122)\n",
        "    \tplt.title('Classification Accuracy')\n",
        "    \tplt.ylabel('accuracy')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_accuracy'], color='red',\n",
        "                 label='test')\n",
        "    \tplt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# les fonctionnes d'authentification et enregistrer les résultat dans fichier csv\n",
        "def authenticate_google_drive():\n",
        "    \"\"\"Authenticate and return the Google Drive API service.\"\"\"\n",
        "    auth.authenticate_user()  # This handles the OAuth2 authentication for Google Colab\n",
        "\n",
        "    # Build the Google Drive API client\n",
        "    service = build('drive', 'v3')\n",
        "    return service\n",
        "\n",
        "def download_file_from_drive(file_id):\n",
        "    \"\"\"Download a file from Google Drive.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Request the file's content\n",
        "    request = service.files().export_media(fileId=file_id, mimeType='text/csv')\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "\n",
        "    fh.seek(0)\n",
        "    return pd.read_csv(fh)\n",
        "\n",
        "def upload_file_to_drive(file_id, file_path):\n",
        "    \"\"\"Upload the file to Google Drive after modification.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Upload the modified file to Google Drive\n",
        "    media = MediaFileUpload(file_path, mimetype='text/csv')\n",
        "    updated_file = service.files().update(fileId=file_id, media_body=media).execute()\n",
        "\n",
        "    print(f\"File updated: {updated_file['name']}\")\n",
        "\n",
        "def append_train_results_to_csv(file_id, train_results):\n",
        "    \"\"\"Append new train results to an existing CSV file on Google Drive.\"\"\"\n",
        "    # Download the existing CSV file\n",
        "\n",
        "    try:\n",
        "        df = download_file_from_drive(file_id)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        # If the file is empty, create an empty DataFrame with the correct columns\n",
        "        column_names = list(train_results.keys())\n",
        "        df = pd.DataFrame(columns=column_names)\n",
        "\n",
        "    # Append the new results (train_results should be a dict)\n",
        "    new_row = pd.DataFrame([train_results])\n",
        "    df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "    # Save the modified DataFrame to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, mode='w', newline='') as tmpfile:\n",
        "        temp_path = tmpfile.name\n",
        "        df.to_csv(temp_path, index=False)\n",
        "\n",
        "    # Upload the modified CSV back to Google Drive\n",
        "    upload_file_to_drive(file_id, temp_path)\n",
        "\n",
        "    # Bloc universel pour sauvegarder modèle et résultats\n",
        "def save_model_and_results(model, model_name, x_train=None, y_train=None, x_val=None, y_val=None,\n",
        "                           x_test=None, y_test=None, history=None, training_time=None,\n",
        "                           batch_size=None, epochs=None, optimizer=None, file_id=None):\n",
        "\n",
        "    # Identifiant unique basé sur la date/heure\n",
        "    training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "    saved_model_filename = f\"{model_name}_{training_id}.joblib\"\n",
        "\n",
        "    # Sauvegarder le modèle avec joblib\n",
        "    model_path = f'/content/drive/MyDrive/models/{saved_model_filename}'\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f'Modèle sauvegardé dans {model_path}')\n",
        "\n",
        "    # Initialiser les métriques\n",
        "    final_loss = None\n",
        "    final_accuracy = None\n",
        "    final_val_loss = None\n",
        "    final_val_accuracy = None\n",
        "    test_accuracy = None\n",
        "\n",
        "    # Cas 1 : Modèles Keras avec history (CNN, ANN, etc.)\n",
        "    if history is not None:\n",
        "        final_loss = history.history.get('loss', [None])[-1]\n",
        "        final_accuracy = history.history.get('accuracy', [None])[-1]\n",
        "        final_val_loss = history.history.get('val_loss', [None])[-1]\n",
        "        final_val_accuracy = history.history.get('val_accuracy', [None])[-1]\n",
        "\n",
        "    # Cas 2 : Modèles scikit-learn (KNN, SVC, etc.) sans history\n",
        "    else:\n",
        "        if x_train is not None and y_train is not None:\n",
        "            final_accuracy = model.score(x_train, y_train) if hasattr(model, 'score') else None\n",
        "        if x_val is not None and y_val is not None:\n",
        "            y_pred_val = model.predict(x_val)\n",
        "            final_val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "        if x_test is not None and y_test is not None:\n",
        "            y_pred_test = model.predict(x_test)\n",
        "            test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    # Remplir le dictionnaire des résultats\n",
        "    train_results = {\n",
        "        'Training ID': training_id,\n",
        "        'Model Name': model_name,\n",
        "        'Coach': 'Reza',\n",
        "        'Epoch': epochs if epochs else None,\n",
        "        'Loss': final_loss,\n",
        "        'Accuracy': final_accuracy,\n",
        "        'Validation Loss': final_val_loss,\n",
        "        'Validation Accuracy': final_val_accuracy,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Training Time (s)': training_time if training_time else None,\n",
        "        'Batch Size': batch_size if batch_size else None,\n",
        "        'Learning Rate': None,  # À ajouter manuellement si pertinent\n",
        "        'Optimizer': optimizer if optimizer else None,\n",
        "        'Model Type': 'Sequential' if history else 'Scikit-learn',\n",
        "        'Dataset Type': 'Custom',\n",
        "        'Saved Model Filename': saved_model_filename\n",
        "    }\n",
        "\n",
        "    # Sauvegarder dans Google Drive\n",
        "    if file_id:\n",
        "        append_train_results_to_csv(file_id, train_results)\n",
        "    else:\n",
        "        print(\"Aucun file_id fourni, résultats non sauvegardés dans Google Drive.\")\n",
        "\n",
        "\n",
        "# Fonction générique pour visualiser les résultats\n",
        "def plot_model_results(model, model_name, x_test, y_test, history=None, class_names=None):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "        model: Modèle entraîné (Keras ou scikit-learn)\n",
        "        model_name: Nom du modèle (str)\n",
        "        x_test: Données de test\n",
        "        y_test: Étiquettes réelles de test\n",
        "        history: Objet history de Keras (optionnel)\n",
        "        class_names: Liste des noms de classes (ex. ['Non-tigre', 'Tigre'])\n",
        "    \"\"\"\n",
        "    # Prédictions\n",
        "    if 'tensorflow' in str(type(model)):  # Modèles Keras\n",
        "        y_pred = model.predict(x_test)\n",
        "        # Si y_pred est un tableau 2D (comme pour CNN avec sigmoid), le convertir en classes\n",
        "        if len(y_pred.shape) > 1 and y_pred.shape[1] == 1:\n",
        "            y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "        else:\n",
        "            y_pred_classes = y_pred.argmax(axis=1)  # Pour multiclasse\n",
        "    else:  # Modèles scikit-learn\n",
        "        y_pred_classes = model.predict(x_test)\n",
        "\n",
        "    # Calcul de la matrice de confusion\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "    # Configuration de la figure\n",
        "    if history:  # Si history existe, 3 sous-graphiques (loss, accuracy, confusion)\n",
        "        plt.figure(figsize=(16, 6))\n",
        "        plt.gcf().subplots_adjust(left=0.125, bottom=0.2, right=1, top=0.9, wspace=0.25, hspace=0)\n",
        "        num_plots = 3\n",
        "    else:  # Sinon, seulement la matrice de confusion\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        num_plots = 1\n",
        "\n",
        "    # Courbes d’apprentissage (si history existe)\n",
        "    if history:\n",
        "        # Loss\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(history.history['loss'], label='Training loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "        plt.title(f'{model_name} - Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='upper left')\n",
        "\n",
        "        # Accuracy\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "        plt.title(f'{model_name} - Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='upper left')\n",
        "\n",
        "        # Matrice de confusion\n",
        "        plt.subplot(1, 3, 3)\n",
        "    else:\n",
        "        # Seulement la matrice de confusion\n",
        "        plt.subplot(1, 1, 1)\n",
        "\n",
        "    # Affichage de la matrice de confusion\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
        "                xticklabels=class_names if class_names else ['Class 0', 'Class 1'],\n",
        "                yticklabels=class_names if class_names else ['Class 0', 'Class 1'])\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDbIEQR-DPw"
      },
      "source": [
        "### Les jeux de données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTUWPaTYm4Tr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3869aca7-3564-4324-8922-f0c6c7355034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-06 14:11:30--  https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip\n",
            "Resolving www.lirmm.fr (www.lirmm.fr)... 193.49.104.251\n",
            "Connecting to www.lirmm.fr (www.lirmm.fr)|193.49.104.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7605545 (7.3M) [application/zip]\n",
            "Saving to: ‘Tiger-Fox-Elephant.zip.18’\n",
            "\n",
            "Tiger-Fox-Elephant. 100%[===================>]   7.25M  4.75MB/s    in 1.5s    \n",
            "\n",
            "2025-03-06 14:11:33 (4.75 MB/s) - ‘Tiger-Fox-Elephant.zip.18’ saved [7605545/7605545]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imhdFg9uoIbs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "#with zipfile.ZipFile(\"Tiger-Fox-Elephant.zip\",\"r\") as zip_ref:\n",
        "    #zip_ref.extractall(\"Data_Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix_SIuG9b9fn"
      },
      "source": [
        "**Créer le jeu de données**\n",
        "\n",
        "Actuellement pour chaque animal nous avons un répertoire qui contient des images positives et un répertoire qui contient des images négatives. Pour pouvoir créer un jeu de données nous devons obtenir X et y. Les fonctions ci-dessous permettent de générer, à partir des répertoires, un jeu de données aléatoire pour X et y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02QZLNadp9A"
      },
      "outputs": [],
      "source": [
        "def create_training_data(path_data, list_classes):\n",
        "  training_data=[]\n",
        "  for classes in list_classes:\n",
        "      path=os.path.join(path_data, classes)\n",
        "      class_num=list_classes.index(classes)\n",
        "      for img in os.listdir(path):\n",
        "        try:\n",
        "          img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_UNCHANGED)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "          training_data.append([new_array, class_num])\n",
        "        except Exception as e:\n",
        "          pass\n",
        "  return training_data\n",
        "\n",
        "def create_X_y (path_data, list_classes):\n",
        "      # récupération des données\n",
        "      training_data=create_training_data(path_data, list_classes)\n",
        "      # tri des données\n",
        "      random.shuffle(training_data)\n",
        "      # création de X et y\n",
        "      X=[]\n",
        "      y=[]\n",
        "      for features, label in training_data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "      X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "      y=np.array(y)\n",
        "      return X,y\n",
        "\n",
        "def plot_examples(X,y):\n",
        "  plt.figure(figsize=(15,15))\n",
        "  for i in range(COLUMNS):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    # cv2 lit met les images en BGR et matplotlib lit du RGB\n",
        "    X[i] = cv2.cvtColor(X[i], cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(X[i]/255.,cmap=plt.cm.binary)\n",
        "    plt.xlabel('classe ' + str(y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9aGBOyCYeUh"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ObM28wYqay"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK1qXTdyeIEF"
      },
      "outputs": [],
      "source": [
        "# constantes globales\n",
        "\n",
        "IMG_SIZE=64\n",
        "COLUMNS = 25 # Nombre d'images à afficher\n",
        "my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "\n",
        "#Pré-requis CNN et ANN\n",
        "optimizer = 'adam'\n",
        "activation='relu'\n",
        "batch_size=32\n",
        "epochs=15\n",
        "\n",
        "# Charger les données\n",
        "X, y =create_X_y(my_path,my_classes)\n",
        "X=X.astype('float')\n",
        "X=X/255.0\n",
        "# Diviser en ensembles d'entraînement, de validation, et de test\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcCfUW3NYu_M"
      },
      "source": [
        "## Modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg8sN0xdOJyU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Pré-requis\n",
        "optimizer = 'adam'\n",
        "activation='relu'\n",
        "batch_size=32\n",
        "epochs=15\n",
        "model_name=\"CNN\"\n",
        "print(f\"Train: {len(x_train)}, Validation: {len(x_val)}, Test: {len(x_test)}\")\n",
        "\n",
        "# === 2. Construire le modèle CNN ===\n",
        "model = models.Sequential([\n",
        "    # Première couche de convolution\n",
        "    layers.Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Deuxième couche de convolution\n",
        "    layers.Conv2D(64, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Troisième couche de convolution\n",
        "    layers.Conv2D(128, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Aplatir les données et ajouter les couches denses\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=activation),\n",
        "    # layers.Dropout(0.5),  # Pour éviter le surapprentissage\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoïde pour classification binaire\n",
        "])\n",
        "\n",
        "# === 3. Compiler et entraîner le modèle ===\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',  # Perte pour la classification binaire\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())  # Résumé du modèle\n",
        "start_time = time.time()\n",
        "\n",
        "# Entraîner le modèle\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "#évaluer performances\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modèle + KFold"
      ],
      "metadata": {
        "id": "f4y4zIfH0A6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name=\"CNN-KFOLD\"\n",
        "\n",
        "\n",
        "\n",
        "# Paramètres pour k-fold\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Liste pour stocker les accuracies de chaque fold\n",
        "accuracies_cnn = []\n",
        "\n",
        "# Fonction pour créer le modèle CNN\n",
        "def create_cnn():\n",
        "    model = models.Sequential([\n",
        "        # Première couche de convolution\n",
        "        layers.Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Deuxième couche de convolution\n",
        "        layers.Conv2D(64, (3, 3), activation=activation),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Troisième couche de convolution\n",
        "        layers.Conv2D(128, (3, 3), activation=activation),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Aplatir les données et ajouter les couches denses\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation=activation),\n",
        "        # layers.Dropout(0.5),  # Décommenter si tu veux éviter le surapprentissage\n",
        "        layers.Dense(1, activation='sigmoid')  # Sigmoïde pour classification binaire\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Validation croisée\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(x_train)):\n",
        "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    print(f\"\\nFold {fold+1}/{k}\")\n",
        "    print(f\"Train: {len(x_train_fold)}, Validation: {len(x_val_fold)}\")\n",
        "\n",
        "    # Créer et entraîner le modèle\n",
        "    model = create_cnn()\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_fold, y_train_fold,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_val_fold, y_val_fold),\n",
        "        verbose=0  # Silence pour éviter trop d’affichage\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Évaluation sur le fold de validation (dernière epoch)\n",
        "    val_loss, val_acc = model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
        "    accuracies_cnn.append(val_acc)\n",
        "    print(f\"Fold {fold+1} - Validation Accuracy: {val_acc * 100:.2f}%, Training Time: {training_time:.2f}s\")\n",
        "\n",
        "# Calcul de la moyenne et de l’écart type\n",
        "mean_acc = np.mean(accuracies_cnn)\n",
        "std_acc = np.std(accuracies_cnn)\n",
        "print(f\"\\nCNN - Moyenne Accuracy (k-fold): {mean_acc * 100:.2f}%\")\n",
        "print(f\"CNN - Écart type Accuracy (k-fold): {std_acc * 100:.2f}%\")\n",
        "\n",
        "# # Évaluation finale sur le jeu de test (optionnel)\n",
        "# model_final = create_cnn()\n",
        "# model_final.fit(x_kfold, y_kfold, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "# test_loss, test_accuracy = model_final.evaluate(x_test, y_test, verbose=0)\n",
        "# print(f\"CNN - Test Accuracy (sur jeu de test séparé): {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "MGIeMo2Z2l-q",
        "outputId": "6cc457db-e579-4efe-fe06-1b2c8fa45210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'KFold' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5190c8e705dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Paramètres pour k-fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Liste pour stocker les accuracies de chaque fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wsKm5KmZH2a"
      },
      "source": [
        "## Visualiser les résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is-EvIbWFVKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "b2234d05-1c2a-4c88-85ad-2fe7189342e8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-da4c24e77ff1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sortie brute du modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0my_pred_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert to class 0 or 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "y_pred = model.predict(x_test)  # Sortie brute du modèle\n",
        "y_pred_classes = []\n",
        "for i in y_pred:\n",
        "    for j in i:\n",
        "      y_pred_classes.append ((j > 0.5).astype(int)) # Convert to class 0 or 1\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test , y_pred_classes)\n",
        "plot_model_results(model=model,\n",
        "    model_name=model_name,\n",
        "    x_test=x_test,\n",
        "    y_test=y_test,\n",
        "    history=history ? history : None,\n",
        "    class_names=my_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8V3KliJNiIGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyyLXzeZZSuQ"
      },
      "source": [
        "## Sauvegarder les modèles et Enregistrer les résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnfB7ZHLF4Tf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Récupérer les résultats de l'entraînement\n",
        "final_loss = history.history['loss'][-1] or None\n",
        "final_accuracy = history.history['accuracy'][-1] or None\n",
        "final_val_loss = history.history['val_loss'][-1] or None\n",
        "final_val_accuracy = history.history['val_accuracy'][-1] or None\n",
        "training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "# Scopes required to access and modify Google Drive files\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# La bibliothèque joblib et donc dump a déjà été importée\n",
        "saved_model_filename = training_id\n",
        "# Sauvegarder le modèle (clf) en utilisant joblib\n",
        "\n",
        "joblib.dump(model,'models/' + saved_model_filename ) # Pas besoin d'ouvrir le fichier en mode binaire\n",
        "# Confirmation que le modèle a été sauvegardé\n",
        "print(f'Modèle sauvegardé dans models/{saved_model_filename}')\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_id = '1sEaqLjJN7lZCKJ2fYbOmiqJIytB8wIpKChZkK32kzCI'  # Replace with your actual Google Drive file ID\n",
        "# Remplir le template avec ces résultats\n",
        "train_results = {\n",
        "    'Training ID': training_id,  # Timestamp comme ID d'entraînement\n",
        "    'Model Name': model_name,  # Nom du modèle\n",
        "    'Coach' : 'Reza',\n",
        "    'Epoch': epochs,  # Nombre d'époques\n",
        "    'Loss': final_loss,\n",
        "    'Accuracy': final_accuracy,\n",
        "    'Validation Loss': final_val_loss,\n",
        "    'Validation Accuracy': final_val_accuracy,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Batch size': batch_size,\n",
        "    'Learning Rate': None,  # Exemples de valeurs pour le learning rate\n",
        "    'Optimizer': optimizer,  # Optimiseur utilisé\n",
        "    'Model Type': 'Sequential',  # Type de modèle\n",
        "    'Dataset Type': 'Custom' , # Type de dataset\n",
        "    'Saved model filename': saved_model_filename\n",
        "}\n",
        "\n",
        "append_train_results_to_csv(file_id, train_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloc universel pour sauvegarder modèle et résultats\n",
        "def save_model_and_results(model, model_name, x_train=None, y_train=None, x_val=None, y_val=None,\n",
        "                           x_test=None, y_test=None, history=None, training_time=None,\n",
        "                           batch_size=None, epochs=None, optimizer=None, file_id=None):\n",
        "\n",
        "    # Identifiant unique basé sur la date/heure\n",
        "    training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "    saved_model_filename = f\"{model_name}_{training_id}.joblib\"\n",
        "\n",
        "    # Sauvegarder le modèle avec joblib\n",
        "    model_path = f'/content/drive/MyDrive/models/{saved_model_filename}'\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f'Modèle sauvegardé dans {model_path}')\n",
        "\n",
        "    # Initialiser les métriques\n",
        "    final_loss = None\n",
        "    final_accuracy = None\n",
        "    final_val_loss = None\n",
        "    final_val_accuracy = None\n",
        "    test_accuracy = None\n",
        "\n",
        "    # Cas 1 : Modèles Keras avec history (CNN, ANN, etc.)\n",
        "    if history is not None:\n",
        "        final_loss = history.history.get('loss', [None])[-1]\n",
        "        final_accuracy = history.history.get('accuracy', [None])[-1]\n",
        "        final_val_loss = history.history.get('val_loss', [None])[-1]\n",
        "        final_val_accuracy = history.history.get('val_accuracy', [None])[-1]\n",
        "\n",
        "    # Cas 2 : Modèles scikit-learn (KNN, SVC, etc.) sans history\n",
        "    else:\n",
        "        if x_train is not None and y_train is not None:\n",
        "            final_accuracy = model.score(x_train, y_train) if hasattr(model, 'score') else None\n",
        "        if x_val is not None and y_val is not None:\n",
        "            y_pred_val = model.predict(x_val)\n",
        "            final_val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "        if x_test is not None and y_test is not None:\n",
        "            y_pred_test = model.predict(x_test)\n",
        "            test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    # Remplir le dictionnaire des résultats\n",
        "    train_results = {\n",
        "        'Training ID': training_id,\n",
        "        'Model Name': model_name,\n",
        "        'Coach': 'Reza',\n",
        "        'Epoch': epochs if epochs else None,\n",
        "        'Loss': final_loss,\n",
        "        'Accuracy': final_accuracy,\n",
        "        'Validation Loss': final_val_loss,\n",
        "        'Validation Accuracy': final_val_accuracy,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Training Time (s)': training_time if training_time else None,\n",
        "        'Batch Size': batch_size if batch_size else None,\n",
        "        'Learning Rate': None,  # À ajouter manuellement si pertinent\n",
        "        'Optimizer': optimizer if optimizer else None,\n",
        "        'Model Type': 'Sequential' if history else 'Scikit-learn',\n",
        "        'Dataset Type': 'Custom',\n",
        "        'Saved Model Filename': saved_model_filename\n",
        "    }\n",
        "\n",
        "    # Sauvegarder dans Google Drive\n",
        "    if file_id:\n",
        "        append_train_results_to_csv(file_id, train_results)\n",
        "    else:\n",
        "        print(\"Aucun file_id fourni, résultats non sauvegardés dans Google Drive.\")"
      ],
      "metadata": {
        "id": "Vd9meeUogTET"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ccUTUEh5Xese",
        "fEVOHu-UcrOU",
        "NWgks_L2XyT4"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}