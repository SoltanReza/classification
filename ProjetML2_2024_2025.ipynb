{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoltanReza/classification/blob/main/ProjetML2_2024_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccUTUEh5Xese"
      },
      "source": [
        "# Prérequis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEVOHu-UcrOU"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1zULJ9daqP6"
      },
      "outputs": [],
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd\n",
        "\n",
        "\n",
        "!pip install umap-learn[plot]\n",
        "!pip install holoviews\n",
        "!pip install  ipykernel\n",
        "!pip install joblib\n",
        "# eventuellement ne pas oublier de relancer le kernel du notebook\n",
        "# !pip install tensorflow==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "outputs": [],
      "source": [
        "# Importation des différentes librairies utiles pour le notebook\n",
        "\n",
        "#Sickit learn met régulièrement à jour des versions et\n",
        "#indique des futurs warnings.\n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# librairies générales\n",
        "import joblib\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "from google.colab import auth\n",
        "import google.auth\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "# TensorFlow et keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import glob\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
        "import io\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWgks_L2XyT4"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n70qVaRWwII"
      },
      "outputs": [],
      "source": [
        "# afficher les res de train avec des plots ( version améliorée)\n",
        "def plot_curves_confusion (history,confusion_matrix,class_names):\n",
        "  plt.figure(1,figsize=(16,6))\n",
        "  plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "\n",
        "  # division de la fenêtre graphique en 1 ligne, 3 colonnes,\n",
        "  # graphique en position 1 - loss fonction\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "  # graphique en position 2 - accuracy\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "\n",
        "  # matrice de correlation\n",
        "  plt.subplot(1,3,3)\n",
        "  sns.heatmap(confusion_matrix,annot=True,fmt=\"d\",cmap='Blues',xticklabels=class_names, yticklabels=class_names)# label=class_names)\n",
        "  # labels, title and ticks\n",
        "  plt.xlabel('Predicted', fontsize=12)\n",
        "  #plt.set_label_position('top')\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  #plt.tick_top()\n",
        "  plt.title(\"Correlation matrix\")\n",
        "  plt.ylabel('True', fontsize=12)\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_curves(histories):\n",
        "    plt.figure(1,figsize=(16,6))\n",
        "    plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "    for i in range(len(histories)):\n",
        "    \t# plot loss\n",
        "    \tplt.subplot(121)\n",
        "    \tplt.title('Cross Entropy Loss')\n",
        "    \tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_loss'], color='red', label='test')\n",
        "    \tplt.ylabel('loss')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "    \t# plot accuracy\n",
        "    \tplt.subplot(122)\n",
        "    \tplt.title('Classification Accuracy')\n",
        "    \tplt.ylabel('accuracy')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_accuracy'], color='red',\n",
        "                 label='test')\n",
        "    \tplt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# les fonctionnes d'authentification et enregistrer les résultat dans fichier csv\n",
        "def authenticate_google_drive():\n",
        "    \"\"\"Authenticate and return the Google Drive API service.\"\"\"\n",
        "    auth.authenticate_user()  # This handles the OAuth2 authentication for Google Colab\n",
        "\n",
        "    # Build the Google Drive API client\n",
        "    service = build('drive', 'v3')\n",
        "    return service\n",
        "\n",
        "def download_file_from_drive(file_id):\n",
        "    \"\"\"Download a file from Google Drive.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Request the file's content\n",
        "    request = service.files().export_media(fileId=file_id, mimeType='text/csv')\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "\n",
        "    fh.seek(0)\n",
        "    return pd.read_csv(fh)\n",
        "\n",
        "def upload_file_to_drive(file_id, file_path):\n",
        "    \"\"\"Upload the file to Google Drive after modification.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Upload the modified file to Google Drive\n",
        "    media = MediaFileUpload(file_path, mimetype='text/csv')\n",
        "    updated_file = service.files().update(fileId=file_id, media_body=media).execute()\n",
        "\n",
        "    print(f\"File updated: {updated_file['name']}\")\n",
        "\n",
        "def append_train_results_to_csv(file_id, train_results):\n",
        "    \"\"\"Append new train results to an existing CSV file on Google Drive.\"\"\"\n",
        "    # Download the existing CSV file\n",
        "\n",
        "    try:\n",
        "        df = download_file_from_drive(file_id)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        # If the file is empty, create an empty DataFrame with the correct columns\n",
        "        column_names = list(train_results.keys())\n",
        "        df = pd.DataFrame(columns=column_names)\n",
        "\n",
        "    # Append the new results (train_results should be a dict)\n",
        "    new_row = pd.DataFrame([train_results])\n",
        "    df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "    # Save the modified DataFrame to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, mode='w', newline='') as tmpfile:\n",
        "        temp_path = tmpfile.name\n",
        "        df.to_csv(temp_path, index=False)\n",
        "\n",
        "    # Upload the modified CSV back to Google Drive\n",
        "    upload_file_to_drive(file_id, temp_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDbIEQR-DPw"
      },
      "source": [
        "### Les jeux de données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTUWPaTYm4Tr"
      },
      "outputs": [],
      "source": [
        "!wget https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imhdFg9uoIbs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "#with zipfile.ZipFile(\"Tiger-Fox-Elephant.zip\",\"r\") as zip_ref:\n",
        "    #zip_ref.extractall(\"Data_Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix_SIuG9b9fn"
      },
      "source": [
        "**Créer le jeu de données**\n",
        "\n",
        "Actuellement pour chaque animal nous avons un répertoire qui contient des images positives et un répertoire qui contient des images négatives. Pour pouvoir créer un jeu de données nous devons obtenir X et y. Les fonctions ci-dessous permettent de générer, à partir des répertoires, un jeu de données aléatoire pour X et y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02QZLNadp9A"
      },
      "outputs": [],
      "source": [
        "def create_training_data(path_data, list_classes):\n",
        "  training_data=[]\n",
        "  for classes in list_classes:\n",
        "      path=os.path.join(path_data, classes)\n",
        "      class_num=list_classes.index(classes)\n",
        "      for img in os.listdir(path):\n",
        "        try:\n",
        "          img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_UNCHANGED)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "          training_data.append([new_array, class_num])\n",
        "        except Exception as e:\n",
        "          pass\n",
        "  return training_data\n",
        "\n",
        "def create_X_y (path_data, list_classes):\n",
        "      # récupération des données\n",
        "      training_data=create_training_data(path_data, list_classes)\n",
        "      # tri des données\n",
        "      random.shuffle(training_data)\n",
        "      # création de X et y\n",
        "      X=[]\n",
        "      y=[]\n",
        "      for features, label in training_data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "      X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "      y=np.array(y)\n",
        "      return X,y\n",
        "\n",
        "def plot_examples(X,y):\n",
        "  plt.figure(figsize=(15,15))\n",
        "  for i in range(COLUMNS):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    # cv2 lit met les images en BGR et matplotlib lit du RGB\n",
        "    X[i] = cv2.cvtColor(X[i], cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(X[i]/255.,cmap=plt.cm.binary)\n",
        "    plt.xlabel('classe ' + str(y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9aGBOyCYeUh"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ObM28wYqay"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK1qXTdyeIEF"
      },
      "outputs": [],
      "source": [
        "# constantes globales\n",
        "\n",
        "IMG_SIZE=128\n",
        "COLUMNS = 25 # Nombre d'images à afficher\n",
        "my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcCfUW3NYu_M"
      },
      "source": [
        "## Modèles"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "n65xn5oHlLEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg8sN0xdOJyU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Pré-requis\n",
        "optimizer = 'adam'\n",
        "activation='relu'\n",
        "batch_size=32\n",
        "epochs=15\n",
        "# Charger les données\n",
        "X, y =create_X_y(my_path,my_classes)\n",
        "X=X.astype('float')\n",
        "X=X/255.0\n",
        "# Diviser en ensembles d'entraînement, de validation, et de test\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(x_train)}, Validation: {len(x_val)}, Test: {len(x_test)}\")\n",
        "\n",
        "# === 2. Construire le modèle CNN ===\n",
        "model = models.Sequential([\n",
        "    # Première couche de convolution\n",
        "    layers.Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Deuxième couche de convolution\n",
        "    layers.Conv2D(64, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Troisième couche de convolution\n",
        "    layers.Conv2D(128, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Aplatir les données et ajouter les couches denses\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=activation),\n",
        "    # layers.Dropout(0.5),  # Pour éviter le surapprentissage\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoïde pour classification binaire\n",
        "])\n",
        "\n",
        "# === 3. Compiler et entraîner le modèle ===\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',  # Perte pour la classification binaire\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())  # Résumé du modèle\n",
        "start_time = time.time()\n",
        "\n",
        "# Entraîner le modèle\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "#évaluer performances\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KNN"
      ],
      "metadata": {
        "id": "R1MiojOM-AIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y =create_X_y(my_path,my_classes)\n",
        "x_flattened = X.reshape(X.shape[0], -1)\n",
        "# Normaliser les données\n",
        "x_flattened=x_flattened.astype('float')\n",
        "x_flattened=x_flattened/255.0\n",
        "# Split into train and temp sets (70% train, 30% temp)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(\n",
        "    x_flattened, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Split the temp set into validation and test sets (50% each, 15% of original data)\n",
        "x_val, x_test, y_val, y_test = train_test_split(\n",
        "    x_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Création du modèle KNN avec k=3\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Entraînement du modèle\n",
        "history = model.fit(x_train, y_train)\n",
        "\n",
        "# Prédiction sur les images de test\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "#évaluer les perfomances\n",
        "accuracy = model.score(x_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "y_pred_val = model.predict(x_val)\n",
        "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "print(f\"Validation Accuracy: {accuracy_val * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ZDdzAMR29-2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wsKm5KmZH2a"
      },
      "source": [
        "## Visualiser les résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is-EvIbWFVKR"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred = model.predict(x_test)  # Sortie brute du modèle\n",
        "y_pred_classes = []\n",
        "for i in y_pred:\n",
        "    for j in i:\n",
        "      y_pred_classes.append ((j > 0.5).astype(int)) # Convert to class 0 or 1\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test , y_pred_classes)\n",
        "\n",
        "plot_curves_confusion(history,cm,my_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8V3KliJNiIGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyyLXzeZZSuQ"
      },
      "source": [
        "## Sauvegarder les modèles et Enregistrer les résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnfB7ZHLF4Tf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Récupérer les résultats de l'entraînement\n",
        "final_loss = history.history['loss'][-1] or None\n",
        "final_accuracy = history.history['accuracy'][-1] or None\n",
        "final_val_loss = history.history['val_loss'][-1] or None\n",
        "final_val_accuracy = history.history['val_accuracy'][-1] or None\n",
        "training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "# Scopes required to access and modify Google Drive files\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# La bibliothèque joblib et donc dump a déjà été importée\n",
        "saved_model_filename = training_id\n",
        "# Sauvegarder le modèle (clf) en utilisant joblib\n",
        "\n",
        "joblib.dump(model,'models/' + saved_model_filename ) # Pas besoin d'ouvrir le fichier en mode binaire\n",
        "# Confirmation que le modèle a été sauvegardé\n",
        "print(f'Modèle sauvegardé dans models/{saved_model_filename}')\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_id = '1sEaqLjJN7lZCKJ2fYbOmiqJIytB8wIpKChZkK32kzCI'  # Replace with your actual Google Drive file ID\n",
        "# Remplir le template avec ces résultats\n",
        "train_results = {\n",
        "    'Training ID': training_id,  # Timestamp comme ID d'entraînement\n",
        "    'Model Name': 'CNN',  # Nom du modèle\n",
        "    'Coach' : 'Reza',\n",
        "    'Epoch': epochs,  # Nombre d'époques\n",
        "    'Loss': final_loss,\n",
        "    'Accuracy': final_accuracy,\n",
        "    'Validation Loss': final_val_loss,\n",
        "    'Validation Accuracy': final_val_accuracy,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Batch size': batch_size,\n",
        "    'Learning Rate': None,  # Exemples de valeurs pour le learning rate\n",
        "    'Optimizer': optimizer,  # Optimiseur utilisé\n",
        "    'Model Type': 'Sequential',  # Type de modèle\n",
        "    'Dataset Type': 'Custom' , # Type de dataset\n",
        "    'Saved model filename': saved_model_filename\n",
        "}\n",
        "\n",
        "append_train_results_to_csv(file_id, train_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vd9meeUogTET"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ccUTUEh5Xese",
        "fEVOHu-UcrOU",
        "NWgks_L2XyT4"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}