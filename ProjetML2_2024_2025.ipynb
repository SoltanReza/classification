{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoltanReza/classification/blob/dev/ProjetML2_2024_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccUTUEh5Xese"
      },
      "source": [
        "# PrÃ©requis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEVOHu-UcrOU"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1zULJ9daqP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5c072d-aff8-4675-a2b0-ec1424c52a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/ML_FDS\n",
            "Requirement already satisfied: umap-learn[plot] in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.61.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (3.10.0)\n",
            "Requirement already satisfied: datashader in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.17.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (3.6.3)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (1.20.1)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.13.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from umap-learn[plot]) (0.25.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn[plot]) (0.44.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn[plot]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn[plot]) (3.5.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (1.3.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (24.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh->umap-learn[plot]) (2025.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->umap-learn[plot]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->umap-learn[plot]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->umap-learn[plot]) (2025.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (2.2.0)\n",
            "Requirement already satisfied: pyct in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (0.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (2.32.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from datashader->umap-learn[plot]) (2025.1.2)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.11/dist-packages (from holoviews->umap-learn[plot]) (1.6.1)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.11/dist-packages (from holoviews->umap-learn[plot]) (3.0.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->umap-learn[plot]) (3.2.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->umap-learn[plot]) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (3.0.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->umap-learn[plot]) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->datashader->umap-learn[plot]) (2025.1.31)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.0->holoviews->umap-learn[plot]) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.0->holoviews->umap-learn[plot]) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.0->holoviews->umap-learn[plot]) (0.1.2)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.11/dist-packages (from holoviews) (3.6.3)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from holoviews) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from holoviews) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from holoviews) (24.2)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.11/dist-packages (from holoviews) (2.2.2)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.11/dist-packages (from holoviews) (1.6.1)\n",
            "Requirement already satisfied: param<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from holoviews) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.11/dist-packages (from holoviews) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (1.3.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1->holoviews) (2025.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->holoviews) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->holoviews) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->holoviews) (2025.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (0.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from panel>=1.0->holoviews) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh>=3.1->holoviews) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3->holoviews) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.0->holoviews) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.0->holoviews) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.0->holoviews) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->panel>=1.0->holoviews) (2025.1.31)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (6.17.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n",
        "# Ajout du path pour les librairies, fonctions et donnÃ©es\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le rÃ©pertoire associÃ©\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd\n",
        "\n",
        "\n",
        "!pip install umap-learn[plot]\n",
        "!pip install holoviews\n",
        "!pip install  ipykernel\n",
        "!pip install joblib\n",
        "# eventuellement ne pas oublier de relancer le kernel du notebook\n",
        "# !pip install tensorflow==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "outputs": [],
      "source": [
        "# Importation des diffÃ©rentes librairies utiles pour le notebook\n",
        "\n",
        "#Sickit learn met rÃ©guliÃ¨rement Ã  jour des versions et\n",
        "#indique des futurs warnings.\n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# librairies gÃ©nÃ©rales\n",
        "import joblib\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import auth\n",
        "import google.auth\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "# TensorFlow et keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import glob\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
        "import io\n",
        "import tempfile\n",
        "from skimage.feature import hog\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWgks_L2XyT4"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n70qVaRWwII"
      },
      "outputs": [],
      "source": [
        "# afficher les res de train avec des plots ( version amÃ©liorÃ©e)\n",
        "def plot_curves_confusion (history,confusion_matrix,class_names):\n",
        "  plt.figure(1,figsize=(16,6))\n",
        "  plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "\n",
        "  # division de la fenÃªtre graphique en 1 ligne, 3 colonnes,\n",
        "  # graphique en position 1 - loss fonction\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "  # graphique en position 2 - accuracy\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "\n",
        "  # matrice de correlation\n",
        "  plt.subplot(1,3,3)\n",
        "  sns.heatmap(confusion_matrix,annot=True,fmt=\"d\",cmap='Blues',xticklabels=class_names, yticklabels=class_names)# label=class_names)\n",
        "  # labels, title and ticks\n",
        "  plt.xlabel('Predicted', fontsize=12)\n",
        "  #plt.set_label_position('top')\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  #plt.tick_top()\n",
        "  plt.title(\"Correlation matrix\")\n",
        "  plt.ylabel('True', fontsize=12)\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_curves(histories):\n",
        "    plt.figure(1,figsize=(16,6))\n",
        "    plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "    for i in range(len(histories)):\n",
        "    \t# plot loss\n",
        "    \tplt.subplot(121)\n",
        "    \tplt.title('Cross Entropy Loss')\n",
        "    \tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_loss'], color='red', label='test')\n",
        "    \tplt.ylabel('loss')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "    \t# plot accuracy\n",
        "    \tplt.subplot(122)\n",
        "    \tplt.title('Classification Accuracy')\n",
        "    \tplt.ylabel('accuracy')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_accuracy'], color='red',\n",
        "                 label='test')\n",
        "    \tplt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# les fonctionnes d'authentification et enregistrer les rÃ©sultat dans fichier csv\n",
        "def authenticate_google_drive():\n",
        "    \"\"\"Authenticate and return the Google Drive API service.\"\"\"\n",
        "    auth.authenticate_user()  # This handles the OAuth2 authentication for Google Colab\n",
        "\n",
        "    # Build the Google Drive API client\n",
        "    service = build('drive', 'v3')\n",
        "    return service\n",
        "\n",
        "def download_file_from_drive(file_id):\n",
        "    \"\"\"Download a file from Google Drive.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Request the file's content\n",
        "    request = service.files().export_media(fileId=file_id, mimeType='text/csv')\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "\n",
        "    fh.seek(0)\n",
        "    return pd.read_csv(fh)\n",
        "\n",
        "def upload_file_to_drive(file_id, file_path):\n",
        "    \"\"\"Upload the file to Google Drive after modification.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Upload the modified file to Google Drive\n",
        "    media = MediaFileUpload(file_path, mimetype='text/csv')\n",
        "    updated_file = service.files().update(fileId=file_id, media_body=media).execute()\n",
        "\n",
        "    print(f\"File updated: {updated_file['name']}\")\n",
        "\n",
        "def append_train_results_to_csv(file_id, train_results):\n",
        "    \"\"\"Append new train results to an existing CSV file on Google Drive.\"\"\"\n",
        "    # Download the existing CSV file\n",
        "\n",
        "    try:\n",
        "        df = download_file_from_drive(file_id)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        # If the file is empty, create an empty DataFrame with the correct columns\n",
        "        column_names = list(train_results.keys())\n",
        "        df = pd.DataFrame(columns=column_names)\n",
        "\n",
        "    # Append the new results (train_results should be a dict)\n",
        "    new_row = pd.DataFrame([train_results])\n",
        "    df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "    # Save the modified DataFrame to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, mode='w', newline='') as tmpfile:\n",
        "        temp_path = tmpfile.name\n",
        "        df.to_csv(temp_path, index=False)\n",
        "\n",
        "    # Upload the modified CSV back to Google Drive\n",
        "    upload_file_to_drive(file_id, temp_path)\n",
        "\n",
        "    # Bloc universel pour sauvegarder modÃ¨le et rÃ©sultats\n",
        "def save_model_and_results(model, model_name, x_train=None, y_train=None, x_val=None, y_val=None,\n",
        "                           x_test=None, y_test=None, history=None, training_time=None,\n",
        "                           batch_size=None, epochs=None, optimizer=None, file_id=None):\n",
        "\n",
        "    # Identifiant unique basÃ© sur la date/heure\n",
        "    training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "    saved_model_filename = f\"{model_name}_{training_id}.joblib\"\n",
        "\n",
        "    # Sauvegarder le modÃ¨le avec joblib\n",
        "    model_path = f'/content/drive/MyDrive/models/{saved_model_filename}'\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f'ModÃ¨le sauvegardÃ© dans {model_path}')\n",
        "\n",
        "    # Initialiser les mÃ©triques\n",
        "    final_loss = None\n",
        "    final_accuracy = None\n",
        "    final_val_loss = None\n",
        "    final_val_accuracy = None\n",
        "    test_accuracy = None\n",
        "\n",
        "    # Cas 1 : ModÃ¨les Keras avec history (CNN, ANN, etc.)\n",
        "    if history is not None:\n",
        "        final_loss = history.history.get('loss', [None])[-1]\n",
        "        final_accuracy = history.history.get('accuracy', [None])[-1]\n",
        "        final_val_loss = history.history.get('val_loss', [None])[-1]\n",
        "        final_val_accuracy = history.history.get('val_accuracy', [None])[-1]\n",
        "\n",
        "    # Cas 2 : ModÃ¨les scikit-learn (KNN, SVC, etc.) sans history\n",
        "    else:\n",
        "        if x_train is not None and y_train is not None:\n",
        "            final_accuracy = model.score(x_train, y_train) if hasattr(model, 'score') else None\n",
        "        if x_val is not None and y_val is not None:\n",
        "            y_pred_val = model.predict(x_val)\n",
        "            final_val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "        if x_test is not None and y_test is not None:\n",
        "            y_pred_test = model.predict(x_test)\n",
        "            test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    # Remplir le dictionnaire des rÃ©sultats\n",
        "    train_results = {\n",
        "        'Training ID': training_id,\n",
        "        'Model Name': model_name,\n",
        "        'Coach': 'Reza',\n",
        "        'Epoch': epochs if epochs else None,\n",
        "        'Loss': final_loss,\n",
        "        'Accuracy': final_accuracy,\n",
        "        'Validation Loss': final_val_loss,\n",
        "        'Validation Accuracy': final_val_accuracy,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Training Time (s)': training_time if training_time else None,\n",
        "        'Batch Size': batch_size if batch_size else None,\n",
        "        'Learning Rate': None,  # Ã ajouter manuellement si pertinent\n",
        "        'Optimizer': optimizer if optimizer else None,\n",
        "        'Model Type': 'Sequential' if history else 'Scikit-learn',\n",
        "        'Dataset Type': 'Custom',\n",
        "        'Saved Model Filename': saved_model_filename\n",
        "    }\n",
        "\n",
        "    # Sauvegarder dans Google Drive\n",
        "    if file_id:\n",
        "        append_train_results_to_csv(file_id, train_results)\n",
        "    else:\n",
        "        print(\"Aucun file_id fourni, rÃ©sultats non sauvegardÃ©s dans Google Drive.\")\n",
        "\n",
        "\n",
        "# Fonction gÃ©nÃ©rique pour visualiser les rÃ©sultats\n",
        "def plot_model_results(model, model_name, x_test, y_test, history=None, class_names=None):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "        model: ModÃ¨le entraÃ®nÃ© (Keras ou scikit-learn)\n",
        "        model_name: Nom du modÃ¨le (str)\n",
        "        x_test: DonnÃ©es de test\n",
        "        y_test: Ãtiquettes rÃ©elles de test\n",
        "        history: Objet history de Keras (optionnel)\n",
        "        class_names: Liste des noms de classes (ex. ['Non-tigre', 'Tigre'])\n",
        "    \"\"\"\n",
        "    # PrÃ©dictions\n",
        "    if 'tensorflow' in str(type(model)):  # ModÃ¨les Keras\n",
        "        y_pred = model.predict(x_test)\n",
        "        # Si y_pred est un tableau 2D (comme pour CNN avec sigmoid), le convertir en classes\n",
        "        if len(y_pred.shape) > 1 and y_pred.shape[1] == 1:\n",
        "            y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "        else:\n",
        "            y_pred_classes = y_pred.argmax(axis=1)  # Pour multiclasse\n",
        "    else:  # ModÃ¨les scikit-learn\n",
        "        y_pred_classes = model.predict(x_test)\n",
        "\n",
        "    # Calcul de la matrice de confusion\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "    # Configuration de la figure\n",
        "    if history:  # Si history existe, 3 sous-graphiques (loss, accuracy, confusion)\n",
        "        plt.figure(figsize=(16, 6))\n",
        "        plt.gcf().subplots_adjust(left=0.125, bottom=0.2, right=1, top=0.9, wspace=0.25, hspace=0)\n",
        "        num_plots = 3\n",
        "    else:  # Sinon, seulement la matrice de confusion\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        num_plots = 1\n",
        "\n",
        "    # Courbes dâapprentissage (si history existe)\n",
        "    if history:\n",
        "        # Loss\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(history.history['loss'], label='Training loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "        plt.title(f'{model_name} - Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='upper left')\n",
        "\n",
        "        # Accuracy\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "        plt.title(f'{model_name} - Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(loc='upper left')\n",
        "\n",
        "        # Matrice de confusion\n",
        "        plt.subplot(1, 3, 3)\n",
        "    else:\n",
        "        # Seulement la matrice de confusion\n",
        "        plt.subplot(1, 1, 1)\n",
        "\n",
        "    # Affichage de la matrice de confusion\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
        "                xticklabels=class_names if class_names else ['Class 0', 'Class 1'],\n",
        "                yticklabels=class_names if class_names else ['Class 0', 'Class 1'])\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDbIEQR-DPw"
      },
      "source": [
        "### Les jeux de donnÃ©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTUWPaTYm4Tr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3869aca7-3564-4324-8922-f0c6c7355034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-06 14:11:30--  https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip\n",
            "Resolving www.lirmm.fr (www.lirmm.fr)... 193.49.104.251\n",
            "Connecting to www.lirmm.fr (www.lirmm.fr)|193.49.104.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7605545 (7.3M) [application/zip]\n",
            "Saving to: âTiger-Fox-Elephant.zip.18â\n",
            "\n",
            "Tiger-Fox-Elephant. 100%[===================>]   7.25M  4.75MB/s    in 1.5s    \n",
            "\n",
            "2025-03-06 14:11:33 (4.75 MB/s) - âTiger-Fox-Elephant.zip.18â saved [7605545/7605545]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imhdFg9uoIbs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "#with zipfile.ZipFile(\"Tiger-Fox-Elephant.zip\",\"r\") as zip_ref:\n",
        "    #zip_ref.extractall(\"Data_Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix_SIuG9b9fn"
      },
      "source": [
        "**CrÃ©er le jeu de donnÃ©es**\n",
        "\n",
        "Actuellement pour chaque animal nous avons un rÃ©pertoire qui contient des images positives et un rÃ©pertoire qui contient des images nÃ©gatives. Pour pouvoir crÃ©er un jeu de donnÃ©es nous devons obtenir X et y. Les fonctions ci-dessous permettent de gÃ©nÃ©rer, Ã  partir des rÃ©pertoires, un jeu de donnÃ©es alÃ©atoire pour X et y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02QZLNadp9A"
      },
      "outputs": [],
      "source": [
        "def create_training_data(path_data, list_classes):\n",
        "  training_data=[]\n",
        "  for classes in list_classes:\n",
        "      path=os.path.join(path_data, classes)\n",
        "      class_num=list_classes.index(classes)\n",
        "      for img in os.listdir(path):\n",
        "        try:\n",
        "          img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_UNCHANGED)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "          training_data.append([new_array, class_num])\n",
        "        except Exception as e:\n",
        "          pass\n",
        "  return training_data\n",
        "\n",
        "def create_X_y (path_data, list_classes):\n",
        "      # rÃ©cupÃ©ration des donnÃ©es\n",
        "      training_data=create_training_data(path_data, list_classes)\n",
        "      # tri des donnÃ©es\n",
        "      random.shuffle(training_data)\n",
        "      # crÃ©ation de X et y\n",
        "      X=[]\n",
        "      y=[]\n",
        "      for features, label in training_data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "      X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "      y=np.array(y)\n",
        "      return X,y\n",
        "\n",
        "def plot_examples(X,y):\n",
        "  plt.figure(figsize=(15,15))\n",
        "  for i in range(COLUMNS):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    # cv2 lit met les images en BGR et matplotlib lit du RGB\n",
        "    X[i] = cv2.cvtColor(X[i], cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(X[i]/255.,cmap=plt.cm.binary)\n",
        "    plt.xlabel('classe ' + str(y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9aGBOyCYeUh"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ObM28wYqay"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK1qXTdyeIEF"
      },
      "outputs": [],
      "source": [
        "# constantes globales\n",
        "\n",
        "IMG_SIZE=64\n",
        "COLUMNS = 25 # Nombre d'images Ã  afficher\n",
        "my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "\n",
        "#PrÃ©-requis CNN et ANN\n",
        "optimizer = 'adam'\n",
        "activation='relu'\n",
        "batch_size=32\n",
        "epochs=15\n",
        "\n",
        "# Charger les donnÃ©es\n",
        "X, y =create_X_y(my_path,my_classes)\n",
        "X=X.astype('float')\n",
        "X=X/255.0\n",
        "# Diviser en ensembles d'entraÃ®nement, de validation, et de test\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcCfUW3NYu_M"
      },
      "source": [
        "## ModÃ¨les"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg8sN0xdOJyU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#PrÃ©-requis\n",
        "optimizer = 'adam'\n",
        "activation='relu'\n",
        "batch_size=32\n",
        "epochs=15\n",
        "model_name=\"CNN\"\n",
        "print(f\"Train: {len(x_train)}, Validation: {len(x_val)}, Test: {len(x_test)}\")\n",
        "\n",
        "# === 2. Construire le modÃ¨le CNN ===\n",
        "model = models.Sequential([\n",
        "    # PremiÃ¨re couche de convolution\n",
        "    layers.Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # DeuxiÃ¨me couche de convolution\n",
        "    layers.Conv2D(64, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # TroisiÃ¨me couche de convolution\n",
        "    layers.Conv2D(128, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Aplatir les donnÃ©es et ajouter les couches denses\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=activation),\n",
        "    # layers.Dropout(0.5),  # Pour Ã©viter le surapprentissage\n",
        "    layers.Dense(1, activation='sigmoid')  # SigmoÃ¯de pour classification binaire\n",
        "])\n",
        "\n",
        "# === 3. Compiler et entraÃ®ner le modÃ¨le ===\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',  # Perte pour la classification binaire\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())  # RÃ©sumÃ© du modÃ¨le\n",
        "start_time = time.time()\n",
        "\n",
        "# EntraÃ®ner le modÃ¨le\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "#Ã©valuer performances\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ModÃ¨le + KFold"
      ],
      "metadata": {
        "id": "f4y4zIfH0A6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name=\"CNN-KFOLD\"\n",
        "\n",
        "\n",
        "\n",
        "# ParamÃ¨tres pour k-fold\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Liste pour stocker les accuracies de chaque fold\n",
        "accuracies_cnn = []\n",
        "\n",
        "# Fonction pour crÃ©er le modÃ¨le CNN\n",
        "def create_cnn():\n",
        "    model = models.Sequential([\n",
        "        # PremiÃ¨re couche de convolution\n",
        "        layers.Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # DeuxiÃ¨me couche de convolution\n",
        "        layers.Conv2D(64, (3, 3), activation=activation),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # TroisiÃ¨me couche de convolution\n",
        "        layers.Conv2D(128, (3, 3), activation=activation),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Aplatir les donnÃ©es et ajouter les couches denses\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation=activation),\n",
        "        # layers.Dropout(0.5),  # DÃ©commenter si tu veux Ã©viter le surapprentissage\n",
        "        layers.Dense(1, activation='sigmoid')  # SigmoÃ¯de pour classification binaire\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Validation croisÃ©e\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(x_train)):\n",
        "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    print(f\"\\nFold {fold+1}/{k}\")\n",
        "    print(f\"Train: {len(x_train_fold)}, Validation: {len(x_val_fold)}\")\n",
        "\n",
        "    # CrÃ©er et entraÃ®ner le modÃ¨le\n",
        "    model = create_cnn()\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_fold, y_train_fold,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_val_fold, y_val_fold),\n",
        "        verbose=0  # Silence pour Ã©viter trop dâaffichage\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Ãvaluation sur le fold de validation (derniÃ¨re epoch)\n",
        "    val_loss, val_acc = model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
        "    accuracies_cnn.append(val_acc)\n",
        "    print(f\"Fold {fold+1} - Validation Accuracy: {val_acc * 100:.2f}%, Training Time: {training_time:.2f}s\")\n",
        "\n",
        "# Calcul de la moyenne et de lâÃ©cart type\n",
        "mean_acc = np.mean(accuracies_cnn)\n",
        "std_acc = np.std(accuracies_cnn)\n",
        "print(f\"\\nCNN - Moyenne Accuracy (k-fold): {mean_acc * 100:.2f}%\")\n",
        "print(f\"CNN - Ãcart type Accuracy (k-fold): {std_acc * 100:.2f}%\")\n",
        "\n",
        "# # Ãvaluation finale sur le jeu de test (optionnel)\n",
        "# model_final = create_cnn()\n",
        "# model_final.fit(x_kfold, y_kfold, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "# test_loss, test_accuracy = model_final.evaluate(x_test, y_test, verbose=0)\n",
        "# print(f\"CNN - Test Accuracy (sur jeu de test sÃ©parÃ©): {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "MGIeMo2Z2l-q",
        "outputId": "6cc457db-e579-4efe-fe06-1b2c8fa45210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'KFold' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5190c8e705dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ParamÃ¨tres pour k-fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Liste pour stocker les accuracies de chaque fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wsKm5KmZH2a"
      },
      "source": [
        "## Visualiser les rÃ©sultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is-EvIbWFVKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "b2234d05-1c2a-4c88-85ad-2fe7189342e8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-da4c24e77ff1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sortie brute du modÃ¨le\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0my_pred_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert to class 0 or 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "y_pred = model.predict(x_test)  # Sortie brute du modÃ¨le\n",
        "y_pred_classes = []\n",
        "for i in y_pred:\n",
        "    for j in i:\n",
        "      y_pred_classes.append ((j > 0.5).astype(int)) # Convert to class 0 or 1\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test , y_pred_classes)\n",
        "plot_model_results(model=model,\n",
        "    model_name=model_name,\n",
        "    x_test=x_test,\n",
        "    y_test=y_test,\n",
        "    history=history ? history : None,\n",
        "    class_names=my_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8V3KliJNiIGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyyLXzeZZSuQ"
      },
      "source": [
        "## Sauvegarder les modÃ¨les et Enregistrer les rÃ©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnfB7ZHLF4Tf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# RÃ©cupÃ©rer les rÃ©sultats de l'entraÃ®nement\n",
        "final_loss = history.history['loss'][-1] or None\n",
        "final_accuracy = history.history['accuracy'][-1] or None\n",
        "final_val_loss = history.history['val_loss'][-1] or None\n",
        "final_val_accuracy = history.history['val_accuracy'][-1] or None\n",
        "training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "# Scopes required to access and modify Google Drive files\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# La bibliothÃ¨que joblib et donc dump a dÃ©jÃ  Ã©tÃ© importÃ©e\n",
        "saved_model_filename = training_id\n",
        "# Sauvegarder le modÃ¨le (clf) en utilisant joblib\n",
        "\n",
        "joblib.dump(model,'models/' + saved_model_filename ) # Pas besoin d'ouvrir le fichier en mode binaire\n",
        "# Confirmation que le modÃ¨le a Ã©tÃ© sauvegardÃ©\n",
        "print(f'ModÃ¨le sauvegardÃ© dans models/{saved_model_filename}')\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_id = '1sEaqLjJN7lZCKJ2fYbOmiqJIytB8wIpKChZkK32kzCI'  # Replace with your actual Google Drive file ID\n",
        "# Remplir le template avec ces rÃ©sultats\n",
        "train_results = {\n",
        "    'Training ID': training_id,  # Timestamp comme ID d'entraÃ®nement\n",
        "    'Model Name': model_name,  # Nom du modÃ¨le\n",
        "    'Coach' : 'Reza',\n",
        "    'Epoch': epochs,  # Nombre d'Ã©poques\n",
        "    'Loss': final_loss,\n",
        "    'Accuracy': final_accuracy,\n",
        "    'Validation Loss': final_val_loss,\n",
        "    'Validation Accuracy': final_val_accuracy,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Batch size': batch_size,\n",
        "    'Learning Rate': None,  # Exemples de valeurs pour le learning rate\n",
        "    'Optimizer': optimizer,  # Optimiseur utilisÃ©\n",
        "    'Model Type': 'Sequential',  # Type de modÃ¨le\n",
        "    'Dataset Type': 'Custom' , # Type de dataset\n",
        "    'Saved model filename': saved_model_filename\n",
        "}\n",
        "\n",
        "append_train_results_to_csv(file_id, train_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloc universel pour sauvegarder modÃ¨le et rÃ©sultats\n",
        "def save_model_and_results(model, model_name, x_train=None, y_train=None, x_val=None, y_val=None,\n",
        "                           x_test=None, y_test=None, history=None, training_time=None,\n",
        "                           batch_size=None, epochs=None, optimizer=None, file_id=None):\n",
        "\n",
        "    # Identifiant unique basÃ© sur la date/heure\n",
        "    training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "    saved_model_filename = f\"{model_name}_{training_id}.joblib\"\n",
        "\n",
        "    # Sauvegarder le modÃ¨le avec joblib\n",
        "    model_path = f'/content/drive/MyDrive/models/{saved_model_filename}'\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f'ModÃ¨le sauvegardÃ© dans {model_path}')\n",
        "\n",
        "    # Initialiser les mÃ©triques\n",
        "    final_loss = None\n",
        "    final_accuracy = None\n",
        "    final_val_loss = None\n",
        "    final_val_accuracy = None\n",
        "    test_accuracy = None\n",
        "\n",
        "    # Cas 1 : ModÃ¨les Keras avec history (CNN, ANN, etc.)\n",
        "    if history is not None:\n",
        "        final_loss = history.history.get('loss', [None])[-1]\n",
        "        final_accuracy = history.history.get('accuracy', [None])[-1]\n",
        "        final_val_loss = history.history.get('val_loss', [None])[-1]\n",
        "        final_val_accuracy = history.history.get('val_accuracy', [None])[-1]\n",
        "\n",
        "    # Cas 2 : ModÃ¨les scikit-learn (KNN, SVC, etc.) sans history\n",
        "    else:\n",
        "        if x_train is not None and y_train is not None:\n",
        "            final_accuracy = model.score(x_train, y_train) if hasattr(model, 'score') else None\n",
        "        if x_val is not None and y_val is not None:\n",
        "            y_pred_val = model.predict(x_val)\n",
        "            final_val_accuracy = accuracy_score(y_val, y_pred_val)\n",
        "        if x_test is not None and y_test is not None:\n",
        "            y_pred_test = model.predict(x_test)\n",
        "            test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    # Remplir le dictionnaire des rÃ©sultats\n",
        "    train_results = {\n",
        "        'Training ID': training_id,\n",
        "        'Model Name': model_name,\n",
        "        'Coach': 'Reza',\n",
        "        'Epoch': epochs if epochs else None,\n",
        "        'Loss': final_loss,\n",
        "        'Accuracy': final_accuracy,\n",
        "        'Validation Loss': final_val_loss,\n",
        "        'Validation Accuracy': final_val_accuracy,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Training Time (s)': training_time if training_time else None,\n",
        "        'Batch Size': batch_size if batch_size else None,\n",
        "        'Learning Rate': None,  # Ã ajouter manuellement si pertinent\n",
        "        'Optimizer': optimizer if optimizer else None,\n",
        "        'Model Type': 'Sequential' if history else 'Scikit-learn',\n",
        "        'Dataset Type': 'Custom',\n",
        "        'Saved Model Filename': saved_model_filename\n",
        "    }\n",
        "\n",
        "    # Sauvegarder dans Google Drive\n",
        "    if file_id:\n",
        "        append_train_results_to_csv(file_id, train_results)\n",
        "    else:\n",
        "        print(\"Aucun file_id fourni, rÃ©sultats non sauvegardÃ©s dans Google Drive.\")"
      ],
      "metadata": {
        "id": "Vd9meeUogTET"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ccUTUEh5Xese",
        "fEVOHu-UcrOU",
        "NWgks_L2XyT4"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}