{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoltanReza/classification/blob/main/ProjetML2_2024_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccUTUEh5Xese"
      },
      "source": [
        "# Prérequis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEVOHu-UcrOU"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1zULJ9daqP6"
      },
      "outputs": [],
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd\n",
        "\n",
        "\n",
        "!pip install umap-learn[plot]\n",
        "!pip install holoviews\n",
        "!pip install  ipykernel\n",
        "!pip install joblib\n",
        "# eventuellement ne pas oublier de relancer le kernel du notebook\n",
        "# !pip install tensorflow==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "outputs": [],
      "source": [
        "# Importation des différentes librairies utiles pour le notebook\n",
        "\n",
        "#Sickit learn met régulièrement à jour des versions et\n",
        "#indique des futurs warnings.\n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# librairies générales\n",
        "import joblib\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "from google.colab import auth\n",
        "import google.auth\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "# TensorFlow et keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import glob\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
        "import io\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWgks_L2XyT4"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n70qVaRWwII"
      },
      "outputs": [],
      "source": [
        "# afficher les res de train avec des plots ( version améliorée)\n",
        "def plot_curves_confusion (history,confusion_matrix,class_names):\n",
        "  plt.figure(1,figsize=(16,6))\n",
        "  plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "\n",
        "  # division de la fenêtre graphique en 1 ligne, 3 colonnes,\n",
        "  # graphique en position 1 - loss fonction\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "  # graphique en position 2 - accuracy\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "\n",
        "  # matrice de correlation\n",
        "  plt.subplot(1,3,3)\n",
        "  sns.heatmap(confusion_matrix,annot=True,fmt=\"d\",cmap='Blues',xticklabels=class_names, yticklabels=class_names)# label=class_names)\n",
        "  # labels, title and ticks\n",
        "  plt.xlabel('Predicted', fontsize=12)\n",
        "  #plt.set_label_position('top')\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  #plt.tick_top()\n",
        "  plt.title(\"Correlation matrix\")\n",
        "  plt.ylabel('True', fontsize=12)\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_curves(histories):\n",
        "    plt.figure(1,figsize=(16,6))\n",
        "    plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "    for i in range(len(histories)):\n",
        "    \t# plot loss\n",
        "    \tplt.subplot(121)\n",
        "    \tplt.title('Cross Entropy Loss')\n",
        "    \tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_loss'], color='red', label='test')\n",
        "    \tplt.ylabel('loss')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "    \t# plot accuracy\n",
        "    \tplt.subplot(122)\n",
        "    \tplt.title('Classification Accuracy')\n",
        "    \tplt.ylabel('accuracy')\n",
        "    \tplt.xlabel('epoch')\n",
        "    \tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "    \tplt.plot(histories[i].history['val_accuracy'], color='red',\n",
        "                 label='test')\n",
        "    \tplt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# les fonctionnes d'authentification et enregistrer les résultat dans fichier csv\n",
        "def authenticate_google_drive():\n",
        "    \"\"\"Authenticate and return the Google Drive API service.\"\"\"\n",
        "    auth.authenticate_user()  # This handles the OAuth2 authentication for Google Colab\n",
        "\n",
        "    # Build the Google Drive API client\n",
        "    service = build('drive', 'v3')\n",
        "    return service\n",
        "\n",
        "def download_file_from_drive(file_id):\n",
        "    \"\"\"Download a file from Google Drive.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Request the file's content\n",
        "    request = service.files().export_media(fileId=file_id, mimeType='text/csv')\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "\n",
        "    fh.seek(0)\n",
        "    return pd.read_csv(fh)\n",
        "\n",
        "def upload_file_to_drive(file_id, file_path):\n",
        "    \"\"\"Upload the file to Google Drive after modification.\"\"\"\n",
        "    service = authenticate_google_drive()\n",
        "\n",
        "    # Upload the modified file to Google Drive\n",
        "    media = MediaFileUpload(file_path, mimetype='text/csv')\n",
        "    updated_file = service.files().update(fileId=file_id, media_body=media).execute()\n",
        "\n",
        "    print(f\"File updated: {updated_file['name']}\")\n",
        "\n",
        "def append_train_results_to_csv(file_id, train_results):\n",
        "    \"\"\"Append new train results to an existing CSV file on Google Drive.\"\"\"\n",
        "    # Download the existing CSV file\n",
        "\n",
        "    try:\n",
        "        df = download_file_from_drive(file_id)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        # If the file is empty, create an empty DataFrame with the correct columns\n",
        "        column_names = list(train_results.keys())\n",
        "        df = pd.DataFrame(columns=column_names)\n",
        "\n",
        "    # Append the new results (train_results should be a dict)\n",
        "    new_row = pd.DataFrame([train_results])\n",
        "    df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "    # Save the modified DataFrame to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, mode='w', newline='') as tmpfile:\n",
        "        temp_path = tmpfile.name\n",
        "        df.to_csv(temp_path, index=False)\n",
        "\n",
        "    # Upload the modified CSV back to Google Drive\n",
        "    upload_file_to_drive(file_id, temp_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDbIEQR-DPw"
      },
      "source": [
        "### Les jeux de données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTUWPaTYm4Tr"
      },
      "outputs": [],
      "source": [
        "!wget https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imhdFg9uoIbs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "#with zipfile.ZipFile(\"Tiger-Fox-Elephant.zip\",\"r\") as zip_ref:\n",
        "    #zip_ref.extractall(\"Data_Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix_SIuG9b9fn"
      },
      "source": [
        "**Créer le jeu de données**\n",
        "\n",
        "Actuellement pour chaque animal nous avons un répertoire qui contient des images positives et un répertoire qui contient des images négatives. Pour pouvoir créer un jeu de données nous devons obtenir X et y. Les fonctions ci-dessous permettent de générer, à partir des répertoires, un jeu de données aléatoire pour X et y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02QZLNadp9A"
      },
      "outputs": [],
      "source": [
        "def create_training_data(path_data, list_classes):\n",
        "  training_data=[]\n",
        "  for classes in list_classes:\n",
        "      path=os.path.join(path_data, classes)\n",
        "      class_num=list_classes.index(classes)\n",
        "      for img in os.listdir(path):\n",
        "        try:\n",
        "          img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_UNCHANGED)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "          training_data.append([new_array, class_num])\n",
        "        except Exception as e:\n",
        "          pass\n",
        "  return training_data\n",
        "\n",
        "def create_X_y (path_data, list_classes):\n",
        "      # récupération des données\n",
        "      training_data=create_training_data(path_data, list_classes)\n",
        "      # tri des données\n",
        "      random.shuffle(training_data)\n",
        "      # création de X et y\n",
        "      X=[]\n",
        "      y=[]\n",
        "      for features, label in training_data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "      X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "      y=np.array(y)\n",
        "      return X,y\n",
        "\n",
        "def plot_examples(X,y):\n",
        "  plt.figure(figsize=(15,15))\n",
        "  for i in range(COLUMNS):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    # cv2 lit met les images en BGR et matplotlib lit du RGB\n",
        "    X[i] = cv2.cvtColor(X[i], cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(X[i]/255.,cmap=plt.cm.binary)\n",
        "    plt.xlabel('classe ' + str(y[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9aGBOyCYeUh"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ObM28wYqay"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK1qXTdyeIEF"
      },
      "outputs": [],
      "source": [
        "# constantes globales\n",
        "\n",
        "IMG_SIZE=128\n",
        "COLUMNS = 25 # Nombre d'images à afficher\n",
        "my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "\n",
        "# Charger les données\n",
        "X, y =create_X_y(my_path,my_classes)\n",
        "X=X.astype('float')\n",
        "X=X/255.0\n",
        "# Diviser en ensembles d'entraînement, de validation, et de test\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcCfUW3NYu_M"
      },
      "source": [
        "## Modèles"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "n65xn5oHlLEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg8sN0xdOJyU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Pré-requis\n",
        "optimizer = 'adam'\n",
        "activation='relu'\n",
        "batch_size=32\n",
        "epochs=15\n",
        "\n",
        "print(f\"Train: {len(x_train)}, Validation: {len(x_val)}, Test: {len(x_test)}\")\n",
        "\n",
        "# === 2. Construire le modèle CNN ===\n",
        "model = models.Sequential([\n",
        "    # Première couche de convolution\n",
        "    layers.Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Deuxième couche de convolution\n",
        "    layers.Conv2D(64, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Troisième couche de convolution\n",
        "    layers.Conv2D(128, (3, 3), activation=activation),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Aplatir les données et ajouter les couches denses\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=activation),\n",
        "    # layers.Dropout(0.5),  # Pour éviter le surapprentissage\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoïde pour classification binaire\n",
        "])\n",
        "\n",
        "# === 3. Compiler et entraîner le modèle ===\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',  # Perte pour la classification binaire\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())  # Résumé du modèle\n",
        "start_time = time.time()\n",
        "\n",
        "# Entraîner le modèle\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "#évaluer performances\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KNN"
      ],
      "metadata": {
        "id": "R1MiojOM-AIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y =create_X_y(my_path,my_classes)\n",
        "x_flattened = X.reshape(X.shape[0], -1)\n",
        "# Normaliser les données\n",
        "x_flattened=x_flattened.astype('float')\n",
        "x_flattened=x_flattened/255.0\n",
        "# Split into train and temp sets (70% train, 30% temp)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(\n",
        "    x_flattened, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Split the temp set into validation and test sets (50% each, 15% of original data)\n",
        "x_val, x_test, y_val, y_test = train_test_split(\n",
        "    x_temp, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Création du modèle KNN avec k=3\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Entraînement du modèle\n",
        "history = model.fit(x_train, y_train)\n",
        "\n",
        "# Prédiction sur les images de test\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "#évaluer les perfomances\n",
        "accuracy = model.score(x_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "y_pred_val = model.predict(x_val)\n",
        "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "print(f\"Validation Accuracy: {accuracy_val * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ZDdzAMR29-2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hog + SVC\n"
      ],
      "metadata": {
        "id": "-rquvaQpywa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.feature import hog\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Extraction des features HOG (Histogram of Oriented Gradients)\n",
        "def extract_hog_features(images):\n",
        "    hog_features = []\n",
        "    for img in images:\n",
        "        feature = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), multichannel=True)\n",
        "        hog_features.append(feature)\n",
        "    return np.array(hog_features)\n",
        "\n",
        "X_train_hog = extract_hog_features(X_train)\n",
        "X_val_hog = extract_hog_features(X_val)\n",
        "X_test_hog = extract_hog_features(X_test)\n",
        "\n",
        "# Entraînement d’un SVC\n",
        "svc = SVC(kernel='rbf', random_state=42)\n",
        "svc.fit(X_train_hog, y_train)\n",
        "\n",
        "# Évaluation\n",
        "y_pred_val = svc.predict(X_val_hog)\n",
        "print(\"Accuracy Validation (HOG + SVC):\", accuracy_score(y_val, y_pred_val))\n",
        "\n",
        "y_pred_test = svc.predict(X_test_hog)\n",
        "print(\"Accuracy Test (HOG + SVC):\", accuracy_score(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "W57dMJduy4C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANN"
      ],
      "metadata": {
        "id": "Zp8nR-Kty_Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "# Aplatir les images pour un ANN\n",
        "input_shape = (64 * 64 * 3,)  # 64x64x3 = 12288\n",
        "\n",
        "model_ann = Sequential([\n",
        "    Flatten(input_shape=(64, 64, 3)),  # Transforme l’image en vecteur\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Classification binaire\n",
        "])\n",
        "\n",
        "model_ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_ann = model_ann.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Évaluation sur le test\n",
        "test_loss, test_acc = model_ann.evaluate(X_test, y_test)\n",
        "print(\"Accuracy Test (ANN):\", test_acc)"
      ],
      "metadata": {
        "id": "EoIwA1lwzBrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VGG16"
      ],
      "metadata": {
        "id": "6rGDyOfRzTSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Charger VGG16 pré-entraîné sans la tête fully connected\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "\n",
        "# Geler les couches de base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Ajouter une tête personnalisée\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_vgg = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model_vgg.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_vgg = model_vgg.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Fine-tuning : dégel de quelques couches (optionnel)\n",
        "for layer in base_model.layers[-4:]:  # Dégeler les 4 dernières couches\n",
        "    layer.trainable = True\n",
        "\n",
        "model_vgg.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_finetune = model_vgg.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Évaluation\n",
        "test_loss, test_acc = model_vgg.evaluate(X_test, y_test)\n",
        "print(\"Accuracy Test (VGG16 Fine-tuned):\", test_acc)"
      ],
      "metadata": {
        "id": "S49T3u6HzV2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modèle + KFold"
      ],
      "metadata": {
        "id": "f4y4zIfH0A6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CNN + KFold"
      ],
      "metadata": {
        "id": "jLB08cId2jgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import time\n",
        "\n",
        "# Pré-requis (tes paramètres)\n",
        "optimizer = 'adam'\n",
        "activation = 'relu'\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "IMG_SIZE = 64  # Remplace par la taille réelle de tes images si différente\n",
        "\n",
        "# Exemple fictif pour X et y (remplace par ton chargement réel)\n",
        "# X, y = create_X_y(my_path, my_classes)\n",
        "X = np.random.rand(1000, IMG_SIZE, IMG_SIZE, 3)  # 1000 images IMG_SIZExIMG_SIZEx3\n",
        "y = np.random.randint(0, 2, 1000)                # Étiquettes binaires (0 = non-tigre, 1 = tigre)\n",
        "X = X.astype('float') / 255.0                    # Normalisation\n",
        "\n",
        "# Division initiale pour garder un jeu de test séparé (70% k-fold, 30% test)\n",
        "x_kfold, x_test, y_kfold, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Paramètres pour k-fold\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Liste pour stocker les accuracies de chaque fold\n",
        "accuracies_cnn = []\n",
        "\n",
        "# Fonction pour créer le modèle CNN\n",
        "def create_cnn():\n",
        "    model = models.Sequential([\n",
        "        # Première couche de convolution\n",
        "        layers.Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Deuxième couche de convolution\n",
        "        layers.Conv2D(64, (3, 3), activation=activation),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Troisième couche de convolution\n",
        "        layers.Conv2D(128, (3, 3), activation=activation),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Aplatir les données et ajouter les couches denses\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation=activation),\n",
        "        # layers.Dropout(0.5),  # Décommenter si tu veux éviter le surapprentissage\n",
        "        layers.Dense(1, activation='sigmoid')  # Sigmoïde pour classification binaire\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Validation croisée\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(x_kfold)):\n",
        "    x_train_fold, x_val_fold = x_kfold[train_index], x_kfold[val_index]\n",
        "    y_train_fold, y_val_fold = y_kfold[train_index], y_kfold[val_index]\n",
        "\n",
        "    print(f\"\\nFold {fold+1}/{k}\")\n",
        "    print(f\"Train: {len(x_train_fold)}, Validation: {len(x_val_fold)}\")\n",
        "\n",
        "    # Créer et entraîner le modèle\n",
        "    model = create_cnn()\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train_fold, y_train_fold,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_val_fold, y_val_fold),\n",
        "        verbose=0  # Silence pour éviter trop d’affichage\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Évaluation sur le fold de validation (dernière epoch)\n",
        "    val_loss, val_acc = model.evaluate(x_val_fold, y_val_fold, verbose=0)\n",
        "    accuracies_cnn.append(val_acc)\n",
        "    print(f\"Fold {fold+1} - Validation Accuracy: {val_acc * 100:.2f}%, Training Time: {training_time:.2f}s\")\n",
        "\n",
        "# Calcul de la moyenne et de l’écart type\n",
        "mean_acc = np.mean(accuracies_cnn)\n",
        "std_acc = np.std(accuracies_cnn)\n",
        "print(f\"\\nCNN - Moyenne Accuracy (k-fold): {mean_acc * 100:.2f}%\")\n",
        "print(f\"CNN - Écart type Accuracy (k-fold): {std_acc * 100:.2f}%\")\n",
        "\n",
        "# Évaluation finale sur le jeu de test (optionnel)\n",
        "model_final = create_cnn()\n",
        "model_final.fit(x_kfold, y_kfold, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "test_loss, test_accuracy = model_final.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"CNN - Test Accuracy (sur jeu de test séparé): {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "MGIeMo2Z2l-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KNN + KFold"
      ],
      "metadata": {
        "id": "64wVKbeB2cl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fonction pour charger les données (supposée définie ailleurs)\n",
        "# X, y = create_X_y(my_path, my_classes)\n",
        "# Pour cet exemple, je simule des données fictives. Remplace par ton appel réel.\n",
        "X = np.random.rand(1000, 64, 64, 3)  # Exemple : 1000 images 64x64x3\n",
        "y = np.random.randint(0, 2, 1000)     # Étiquettes binaires (0 = non-tigre, 1 = tigre)\n",
        "\n",
        "# Aplatir les images et normaliser\n",
        "x_flattened = X.reshape(X.shape[0], -1)\n",
        "x_flattened = x_flattened.astype('float') / 255.0\n",
        "\n",
        "# Division initiale pour garder un jeu de test séparé (optionnel)\n",
        "# 70% pour k-fold, 30% pour test final\n",
        "x_kfold, x_test, y_kfold, y_test = train_test_split(\n",
        "    x_flattened, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Paramètres pour k-fold\n",
        "k = 5  # Nombre de folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Liste pour stocker les accuracies de chaque fold\n",
        "accuracies_knn = []\n",
        "\n",
        "# Validation croisée\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(x_kfold)):\n",
        "    x_train_fold, x_val_fold = x_kfold[train_index], x_kfold[val_index]\n",
        "    y_train_fold, y_val_fold = y_kfold[train_index], y_kfold[val_index]\n",
        "\n",
        "    # Création du modèle KNN avec k=3\n",
        "    model = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "    # Entraînement\n",
        "    model.fit(x_train_fold, y_train_fold)\n",
        "\n",
        "    # Prédiction et évaluation sur le fold de validation\n",
        "    y_pred_val = model.predict(x_val_fold)\n",
        "    acc = accuracy_score(y_val_fold, y_pred_val)\n",
        "    accuracies_knn.append(acc)\n",
        "    print(f\"Fold {fold+1} - KNN Validation Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# Calcul de la moyenne et de l’écart type\n",
        "mean_acc = np.mean(accuracies_knn)\n",
        "std_acc = np.std(accuracies_knn)\n",
        "print(f\"KNN - Moyenne Accuracy (k-fold): {mean_acc * 100:.2f}%\")\n",
        "print(f\"KNN - Écart type Accuracy (k-fold): {std_acc * 100:.2f}%\")\n",
        "\n",
        "# Évaluation finale sur le jeu de test (optionnel)\n",
        "model_final = KNeighborsClassifier(n_neighbors=3)\n",
        "model_final.fit(x_kfold, y_kfold)  # Entraînement sur tout le jeu k-fold\n",
        "y_pred_test = model_final.predict(x_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(f\"KNN - Test Accuracy (sur jeu de test séparé): {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "VGR5TSyF2e3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANN bold text+ KFold"
      ],
      "metadata": {
        "id": "Wiqyi6Tj0G5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "def create_ann():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(64, 64, 3)),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Validation croisée\n",
        "accuracies_ann = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    model = create_ann()\n",
        "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Évaluation\n",
        "    _, acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "    accuracies_ann.append(acc)\n",
        "    print(f\"Fold {fold+1} - ANN Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Résultats\n",
        "mean_acc = np.mean(accuracies_ann)\n",
        "std_acc = np.std(accuracies_ann)\n",
        "print(f\"ANN - Moyenne: {mean_acc:.4f}, Écart type: {std_acc:.4f}\")"
      ],
      "metadata": {
        "id": "I9eB4WXO0MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###HOG + SVC + KFold"
      ],
      "metadata": {
        "id": "CDsfBaG60RlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.feature import hog\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def extract_hog_features(images):\n",
        "    hog_features = []\n",
        "    for img in images:\n",
        "        feature = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), multichannel=True)\n",
        "        hog_features.append(feature)\n",
        "    return np.array(hog_features)\n",
        "\n",
        "# Validation croisée\n",
        "accuracies_hog_svc = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    # Extraction des features HOG\n",
        "    X_train_hog = extract_hog_features(X_train_fold)\n",
        "    X_val_hog = extract_hog_features(X_val_fold)\n",
        "\n",
        "    # Entraînement SVC\n",
        "    svc = SVC(kernel='rbf', random_state=42)\n",
        "    svc.fit(X_train_hog, y_train_fold)\n",
        "\n",
        "    # Prédiction et accuracy\n",
        "    y_pred = svc.predict(X_val_hog)\n",
        "    acc = accuracy_score(y_val_fold, y_pred)\n",
        "    accuracies_hog_svc.append(acc)\n",
        "    print(f\"Fold {fold+1} - HOG+SVC Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Résultats\n",
        "mean_acc = np.mean(accuracies_hog_svc)\n",
        "std_acc = np.std(accuracies_hog_svc)\n",
        "print(f\"HOG+SVC - Moyenne: {mean_acc:.4f}, Écart type: {std_acc:.4f}\")"
      ],
      "metadata": {
        "id": "cYtoYi5J0Wxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VGG16 + KFold"
      ],
      "metadata": {
        "id": "J8fsNGt90g0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_vgg16():\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Validation croisée\n",
        "accuracies_vgg = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    model = create_vgg16()\n",
        "    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Évaluation\n",
        "    _, acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "    accuracies_vgg.append(acc)\n",
        "    print(f\"Fold {fold+1} - VGG16 Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Résultats\n",
        "mean_acc = np.mean(accuracies_vgg)\n",
        "std_acc = np.std(accuracies_vgg)\n",
        "print(f\"VGG16 - Moyenne: {mean_acc:.4f}, Écart type: {std_acc:.4f}\")"
      ],
      "metadata": {
        "id": "mFcZqSCw0nnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gaussian + KFold"
      ],
      "metadata": {
        "id": "UV5IuZSk073Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Validation croisée\n",
        "accuracies_gnb = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    # Aplatir les images\n",
        "    X_train_flat = X_train_fold.reshape(X_train_fold.shape[0], -1)\n",
        "    X_val_flat = X_val_fold.reshape(X_val_fold.shape[0], -1)\n",
        "\n",
        "    # Entraînement\n",
        "    gnb = GaussianNB()\n",
        "    gnb.fit(X_train_flat, y_train_fold)\n",
        "\n",
        "    # Prédiction et accuracy\n",
        "    y_pred = gnb.predict(X_val_flat)\n",
        "    acc = accuracy_score(y_val_fold, y_pred)\n",
        "    accuracies_gnb.append(acc)\n",
        "    print(f\"Fold {fold+1} - GaussianNB Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Résultats\n",
        "mean_acc = np.mean(accuracies_gnb)\n",
        "std_acc = np.std(accuracies_gnb)\n",
        "print(f\"GaussianNB - Moyenne: {mean_acc:.4f}, Écart type: {std_acc:.4f}\")"
      ],
      "metadata": {
        "id": "GKOzcX6x0_-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wsKm5KmZH2a"
      },
      "source": [
        "## Visualiser les résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is-EvIbWFVKR"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred = model.predict(x_test)  # Sortie brute du modèle\n",
        "y_pred_classes = []\n",
        "for i in y_pred:\n",
        "    for j in i:\n",
        "      y_pred_classes.append ((j > 0.5).astype(int)) # Convert to class 0 or 1\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test , y_pred_classes)\n",
        "\n",
        "plot_curves_confusion(history,cm,my_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8V3KliJNiIGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyyLXzeZZSuQ"
      },
      "source": [
        "## Sauvegarder les modèles et Enregistrer les résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnfB7ZHLF4Tf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Récupérer les résultats de l'entraînement\n",
        "final_loss = history.history['loss'][-1] or None\n",
        "final_accuracy = history.history['accuracy'][-1] or None\n",
        "final_val_loss = history.history['val_loss'][-1] or None\n",
        "final_val_accuracy = history.history['val_accuracy'][-1] or None\n",
        "training_id = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "# Scopes required to access and modify Google Drive files\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# La bibliothèque joblib et donc dump a déjà été importée\n",
        "saved_model_filename = training_id\n",
        "# Sauvegarder le modèle (clf) en utilisant joblib\n",
        "\n",
        "joblib.dump(model,'models/' + saved_model_filename ) # Pas besoin d'ouvrir le fichier en mode binaire\n",
        "# Confirmation que le modèle a été sauvegardé\n",
        "print(f'Modèle sauvegardé dans models/{saved_model_filename}')\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_id = '1sEaqLjJN7lZCKJ2fYbOmiqJIytB8wIpKChZkK32kzCI'  # Replace with your actual Google Drive file ID\n",
        "# Remplir le template avec ces résultats\n",
        "train_results = {\n",
        "    'Training ID': training_id,  # Timestamp comme ID d'entraînement\n",
        "    'Model Name': 'CNN',  # Nom du modèle\n",
        "    'Coach' : 'Reza',\n",
        "    'Epoch': epochs,  # Nombre d'époques\n",
        "    'Loss': final_loss,\n",
        "    'Accuracy': final_accuracy,\n",
        "    'Validation Loss': final_val_loss,\n",
        "    'Validation Accuracy': final_val_accuracy,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Batch size': batch_size,\n",
        "    'Learning Rate': None,  # Exemples de valeurs pour le learning rate\n",
        "    'Optimizer': optimizer,  # Optimiseur utilisé\n",
        "    'Model Type': 'Sequential',  # Type de modèle\n",
        "    'Dataset Type': 'Custom' , # Type de dataset\n",
        "    'Saved model filename': saved_model_filename\n",
        "}\n",
        "\n",
        "append_train_results_to_csv(file_id, train_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vd9meeUogTET"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ccUTUEh5Xese",
        "fEVOHu-UcrOU",
        "NWgks_L2XyT4"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}